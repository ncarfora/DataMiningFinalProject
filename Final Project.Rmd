---
title: "Final Project"
author: "Jay Powell, Nikki Carfora"
date: "2024-12-04"
output: html_document
---
# Customer Retention Analysis {.tabset}

## Introduction {.tabset}
This section introduces the business problem, the dataset used, and the goals of the analysis. It briefly explains the significance of customer retention for the telecom industry.

## Required Packages
Here are the packages used in our analysis, including a short description of each:

* tidymodels: For building and validating machine learning models.
* tidyverse: For data manipulation and visualization.
* baguette: For bagged tree models.
* vip: For assessing variable importance in models.
* pdp: For creating partial dependence plots.
* kernlab: For kernel-based methods.
* ggplot2: For data visualization.
```{r, message = FALSE, warning = FALSE}
# Loading Required Libraries
  library(tidymodels)
  library(tidyverse)
  library(baguette)
  library(vip)
  library(pdp)
  library(kernlab)
  library(ggplot2)
```

## Data Preparation  {.tabset}
We begin by importing the dataset and exploring its structure to identify potential data quality issues or necessary transformations.
```{r results='hide'}
# Importing the dataset
customer_retention <- read.csv("Data/customer_retention.csv")

# Viewing the first few rows of the dataset
head(customer_retention)

# Checking the structure of the dataset
str(customer_retention)

# Summary statistics of the dataset
summary(customer_retention)
```

The column TotalCharges contains 11 missing values. We omit these rows to ensure data quality for our analysis.
```{r}
# Identifying missing values
colSums(is.na(customer_retention))

# Removing rows with missing values
customer_retention <- na.omit(customer_retention)

# Confirming all missing values have been removed
sum(is.na(customer_retention))
```

To prevent issues during analysis, we ensure that categorical variables are consistent and have no unexpected levels.
```{r results='hide'}
unique(customer_retention$Status)
unique(customer_retention$Gender)
unique(customer_retention$SeniorCitizen)
unique(customer_retention$Partner)
unique(customer_retention$Dependents)
unique(customer_retention$InternetService)
unique(customer_retention$OnlineSecurity)
unique(customer_retention$OnlineBackup)
unique(customer_retention$DeviceProtection)
unique(customer_retention$TechSupport)
unique(customer_retention$StreamingTV)
unique(customer_retention$StreamingMovies)
unique(customer_retention$Contract)
unique(customer_retention$PaperlessBilling)
```
We split the dataset into training (70%) and test (30%) sets, ensuring stratified sampling to maintain the balance of Status in both sets.
```{r}
# Splitting the dataset
set.seed(123)
retention_split <- initial_split(customer_retention, prop = 0.7, strata = "Status")
retention_train <- training(retention_split)
retention_test <- testing(retention_split)

# Checking dimensions of the splits
dim(retention_train)
dim(retention_test)
```

## Exploratory Data Analysis {.tabset}
### Overview of Target Variable
The Status variable indicates whether a customer is currently retained (Current) or has left (Left). Below is the distribution of the target variable in the dataset.
```{r}
table(customer_retention$Status)
prop.table(table(customer_retention$Status))
```
The majority of customers (73.4%) are current, while just over a quarter (26.6%) have left the service. 

### Continuous Variables
#### Tenure Distribution
We first explored the distribution of tenure among our customer base, both current and past.
```{r}
cust_tenure <- ggplot(customer_retention, aes(x = Tenure, fill = Status)) +
  geom_histogram(binwidth = 5) + 
  theme_grey() +
  scale_x_continuous(n.breaks=10)+
  labs(title = "Customer Tenure", x = "Number of Months Kept", y = "Count of Instances",)
cust_tenure
```

It seems that of the current customer base, the majority have either just joined in the last few months, or have been with the service since the beginning. This could be because the service offered a special deal upon startup that customers had been grandfathered into for those who have been long-standing customers. Those who have left the service departed within the first few months, and many of the current ones are new as well, suggesting there could be a promotion for the first ~6 months of the contract that attracts people, and they may choose to switch services once the promotion is up. 

#### Monthly Charges
In addition to the tenure, we explored monthly charges to understand why customers may leave or stay with the service. 
```{r}
monthly_charges<- ggplot(customer_retention, aes(x = MonthlyCharges, fill = Status)) +
  geom_histogram(binwidth = 3) +
  theme_grey() +
  scale_x_continuous(n.breaks=10)+
  labs(title = "Customer Monthly Charges", x = "Amount Billed ($)", y = "Count of Instances")
monthly_charges
```

In this graph, we find that most of the current customers are being charged around $20/month. If we analyze this in tandem with the previous tenure graph, we can conclude there is some sort of promotion for new members that offers the lower price. Many of the customers who left the service began doing so around the $70/month mark, which we may need to consider in our pricing strategy. 

### Categorical Variables
#### Internet Service
We graphed the internet service variable to help understand why customers may be leaving, and if it had to do with their type of internet service.
```{r}
internet_service <- ggplot(customer_retention, aes(x = InternetService, fill = Status)) + 
  geom_bar() +
  theme_minimal()+
  labs(x = "Internet Service", y = "Count", title = "Status by Internet Service Type")
internet_service
```
In doing this, we found that the majority of customers who left had fiber optic cables as their internet service. We may need to look into enhancing or changing our approach for our fiber optic customers to increase retention. 

#### Senior Citizens
We looked at the demographics of our customer base and found that the distribution between male and female identifying customers were fairly even, so we decided to explore the general age of our customer base. 
```{r}
senior_citizen <- ggplot(customer_retention, aes(x = SeniorCitizen, fill = Status)) + 
  geom_bar() +
  theme_minimal()+
  labs(x = "Senior Status", y = "Count", title = "Customers vs Senior Citizen Status")
senior_citizen
```
Most of the customers, both current and past, have not identified as senior citizens. This could help us in marketing strategies in the future, as we can better cater to our larger base, but know we can work to improve how our service could be advertised to and fit for senior citizens. 

### Relationships Between Variables

#### Demographics and Service
Because the non-senior citizen status was so significant in our demographic, we decided to analyze the differnt services between age groups to understand how our service could better fit the senior citizen group. 
```{r}
senior_service <- ggplot(customer_retention, aes(x = SeniorCitizen, fill = InternetService)) +
  geom_bar(position = "dodge") +
  theme_minimal()+
  labs(x = "Senior Status", y = "Count", title = "Relationship between Age and Internet Service Type")
senior_service
```

Both the senior and non-senior demographics utlizie the fiber optic services for their internet, so Regork may want to focus on potentially enhancing their services for the DSL to gain a larger base from the senior citizens, but make sure they fortify the service they do provide in conjunction with fiber optic, as that seems to be the most popular among the graphs. 

#### Tenure vs Status
This graph helps us to better visualize the range of those staying with the service vs leaving it. 
```{r}
tenure_status <- ggplot(customer_retention, aes(x = Status, y = Tenure)) +
  geom_boxplot(fill = "purple") +
  theme_minimal()
tenure_status
```

Most of the customers that leave do so in under 30 months of commitment, with the mean being around 10 months. Regork can look at what promotions or incentives they can offer around the 10 month mark to increase the perceived value of their service and hopefully retain more customers. 

#### Monthly Charge vs Status
We watned to get a better understanding of the costs related to customers leaving the service and the effect the price range may have on the retention. 
```{r}
monthly_status <- ggplot(customer_retention, aes(x = Status, y = MonthlyCharges)) +
  geom_boxplot(fill = "red") +
  theme_minimal()
monthly_status
```

The customers who have left were generally charged more than those who have stayed, paying an average of around $80/month. If Regork can determine why those costs were not associated with perceived value, they may be able to better retain that customer group. 

## Machine Learning Analysis {.tabset}
### Predicting Likelihood of Churn
This section focuses on predicting customer churn using logistic regression, incorporating customer demographics, contract types, and service usage as predictors. Accurately identifying customers likely to churn allows Regork to take targeted retention actions, such as offering incentives, to mitigate potential losses.

```{r}
set.seed(123)
kfold <- vfold_cv(retention_train, v = 5, strata = "Status")
```

To ensure the model is reliable, I used cross-validation, a method that splits the data into smaller subsets for testing and training multiple times. This helps check how well the model performs.

```{r}
results <- logistic_reg() %>%
  set_engine("glm") %>%
  fit_resamples(Status ~ ., resamples = kfold)
```

I used logistic regression to predict churn. This model analyzes how various customer factors contribute to the likelihood of leaving.

```{r}
collect_metrics(results) %>%
  filter(.metric == "accuracy")
```

On average, the model correctly predicts customer churn about 80% of the time. Its performance is consistent, showing little variation.

```{r}
final_fit <- logistic_reg() %>%
  set_engine("glm") %>%
  fit(Status ~ ., data = retention_train)

tidy(final_fit)
```

Key findings:

* Customers on longer contracts are far less likely to leave, making these contracts a strong retention tool.
* Fiber optic internet users are likelier to leave, suggesting dissatisfaction or competitive pressures.
* Higher charges slightly increase the likelihood of churn, though the effect is small.
* Senior citizens are slightly more likely to leave, suggesting this demographic might need targeted retention efforts.

```{r}
final_fit %>%
  predict(retention_test) %>%
  bind_cols(retention_test %>% select(Status)) %>%
  conf_mat(truth = Status, estimate = .pred_class)
```

The model does a good job of predicting customer behavior overall. However:

* It occasionally misses churners (225 cases), which represents lost opportunities to intervene.
* It also predicts churn for some customers who actually stayed (178 cases), which could result in unnecessary retention efforts.

```{r}
vip::vip(final_fit)
```

The most important factors in predicting churn are contract length and how long customers have been with the company. Other influential factors include billing methods, service preferences, and spending habits. These insights can guide Regork’s retention strategies.

### Predicting High-Risk Customer Churn
In this section, we use a random forest model to identify customers at high risk of churn—those with a churn probability greater than 70%. This allows Regork to take targeted actions to retain these customers and reduce revenue loss.

```{r}
# Create the recipe
model_recipe <- recipe(Status ~ ., data = retention_train)

# Create the Random Forest model object
rf_mod <- rand_forest(
  mode = "classification",
  trees = tune(),
  mtry = tune(),
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity")

# Cross-validation setup
set.seed(123)
kfold <- vfold_cv(retention_train, v = 5, strata = "Status")

```

We started by preparing the random forest model and setting up a cross-validation process. This method ensures that the model is tested multiple times on different subsets of the data, helping to assess its reliability and performance

```{r}
rf_hyper_grid <- grid_regular(
  trees(range = c(50, 800)),
  mtry(range = c(2, 19)),
  min_n(range = c(1, 20)),
  levels = 5
)
```

Next, we created a grid of hyperparameter combinations. Hyperparameters (e.g., number of trees, number of variables used per split) affect how well the model predicts churn. This grid allows the model to test multiple settings to find the best combination.

```{r}
set.seed(123)
rf_results <- tune_grid(
  rf_mod,
  model_recipe,
  resamples = kfold,
  grid = rf_hyper_grid,
  metrics = metric_set(accuracy, roc_auc, mn_log_loss)
)
```

We tuned the model by testing all combinations of hyperparameters from the grid. The model was evaluated on metrics such as accuracy and ROC AUC, which measure its ability to distinguish between churners and non-churners.

```{r}
# Show the best hyperparameters based on ROC AUC
show_best(rf_results, metric = "roc_auc")

# Select the best hyperparameters
rf_best_hyperparameters <- select_best(rf_results, metric = "roc_auc")
```

After tuning, we identified the best-performing hyperparameters based on the ROC AUC metric. These parameters were then used to finalize the random forest model.

```{r}
# Create the final workflow with the best hyperparameters
final_rf_wf <- workflow() %>%
  add_recipe(model_recipe) %>%
  add_model(rf_mod) %>%
  finalize_workflow(rf_best_hyperparameters)

# Fit the final model to the training data
rf_final_fit <- final_rf_wf %>%
  fit(data = retention_train)
```

The finalized model was trained on the complete training dataset, using the best settings determined during the tuning process.

```{r}
# Make predictions with probabilities and classes
rf_predictions <- predict(rf_final_fit, new_data = retention_test, type = "prob") %>%
  bind_cols(predict(rf_final_fit, new_data = retention_test, type = "class")) %>%
  bind_cols(retention_test)

# Filter high-risk customers
high_risk_customers <- rf_predictions %>%
  filter(.pred_Left > 0.7)

# Metrics for high-risk predictions
high_risk_metrics <- high_risk_customers %>%
  metrics(truth = Status, estimate = .pred_class)

print(high_risk_metrics)
```

The model was then used to predict churn probabilities for the test dataset. We focused on customers with a churn probability greater than 70% (high risk). The predictions were 86% accurate for this group, demonstrating strong reliability in identifying high-risk churners.

```{r}
# Plot variable importance
rf_final_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10) +
  labs(
    title = "Top 10 Features Influencing High-Risk Customer Churn",
    x = "Feature",
    y = "Importance"
  ) +
  theme_minimal()
```

Finally, we examined the key factors influencing churn risk using variable importance. The top drivers include:

1. Tenure: Shorter tenure is associated with higher churn risk.
2. Total Charges: Customers with higher total charges are more likely to leave.
3. Contract Type: Month-to-month contracts increase churn likelihood.
